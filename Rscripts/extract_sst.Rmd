---
title: "Download and Merge SST Data to Longline Observer Records"
author: "Kapur maia.kapur@noaa.gov"
date: "14 Feb 2017 Ongoing"
output:
  html_document:
    number_sections: yes
    toc: yes

---
## Loak Packages and Download Data

```{r, echo = TRUE, warning = FALSE, message = FALSE}
## Load Packages & Data
library(lubridate)
library(httr)
library(ggplot2)
library(geosphere)
library(knitr)
library(dplyr)
# opts_chunk$set(out.width='750px', out.height = '900px', dpi=200)
```


## Acquisition of ERRDAP Data
  1.  For the SST data, the Longitude data must be transformed such that there are no negative values (180-deg centered). 
  2.  The query dates *must* be in the range available in the database. It will automatically up-or-down correct, but you can't specify a from or to date that is out of bounds. You have to make sure there is a zero in front of months with only one numeral.
```{r, eval = FALSE, warning = FALSE, message = FALSE}
## data extracted using tool
FILE = "G:\\Assessments\\2022 MLS ASSESS\\MLS_logbk_95_20.csv" #"G:/_LLDP/data/all_length_94_17_nulls.csv"
df0 = read.csv(FILE)
#df0<-df
## recenter coordinates
df0$LON2 = ifelse(df0$LONGITUDE < 0, df0$LONGITUDE*-1, df0$LONGITUDE )

# nrow(df0)- sum(complete.cases(df0[,c(11,17)])) ## how many dud locations
df1 = df0[complete.cases(df0[,c('LATITUDE',"LONGITUDE","LON2")]),]

## add numeral 0 in front of months less than oct

## subsample and create aggregation for easier sampling. this generates a neighborhood of fished lat/long for each month and year combination. this helps us batch query ERRDAP with fewer calls, 
# df = df0[sample(nrow(df0), 500), ]
df = df1
df.agg = aggregate(cbind(LATITUDE,LON2) ~ HAUL_MONTH + HAUL_YEAR, data = df, FUN = mean)
```
# SST, DEG C
## 1994 - Oct 2012 (Use deprecated PFGAC, Monthly @ 0.1 Grid Res)
```{r, eval = F}
## where you want raw sst files to be saved (will be several hundred)
dump.file =  "G:/SST/" #'C:/Users/michelle.sculley/Documents/SST/'
## values thru april 2012 (nothing in between)
df.aggEARLY = subset(df.agg, HAUL_YEAR %in% 1994:2011 | HAUL_MONTH < 05 & HAUL_YEAR == 2012)
## Download historical data until until present; can not specify start date, goes until 2016-04-01 but we subset to match above [could take M Abecassis' word that the overlap is quite consistent]; will need to drop prior values to 1994

## add numeral 0 in front of months less than oct

base.url = 'https://oceanwatch.pifsc.noaa.gov/erddap/griddap/OceanWatch_pfgac_sst_monthly.csv?sst%5B('
for(i in 1:nrow(df.aggEARLY)){
  HAULMO = ifelse(df.aggEARLY$HAUL_MONTH < 11, paste0(0,df.aggEARLY$HAUL_MONTH), df.aggEARLY$HAUL_MONTH)[i] ## designate month to download
  HAULYR = df.aggEARLY$HAUL_YEAR[i]
  HAULLAT = df.aggEARLY$LATITUDE[i]
  HAULLON = df.aggEARLY$LON2[i]
  ## ping server
  GET(paste0(base.url,
             HAULYR,'-',HAULMO,'-01T00:00:00Z):1:(',HAULYR,'-',HAULMO,'-01T00:00:00Z)%5D%5B(',
             HAULLAT-0.2,'):1:(',HAULLAT+0.2,')%5D%5B(',HAULLON-0.2,'):1:(',HAULLON+0.2,')%5D'),
       write_disk(paste0(dump.file,i,'-941012-sst.csv'), overwrite = T))
}
```
2012-2018 (Use GOES POES, Monthly @ 0.5 res)
```{r, eval = F}
p <- proc.time() ## set timer
## Download GOES POES until present; first of each month. The 110112 indicates 01 NOV 2012 to Present. Only call for year & month in question for that row
df.aggLATE = subset(df.agg, HAUL_YEAR %in% 2013:2017 |
                      HAUL_MONTH > 10 & HAUL_YEAR == 2012 |
                      HAUL_MONTH < 04 & HAUL_YEAR == 2018)

base.url = 'https://oceanwatch.pifsc.noaa.gov/erddap/griddap/OceanWatch_goes-poes_sst_monthly.csv?sst%5B('
for(i in 1:nrow(df.aggLATE)){
  HAULMO = ifelse(df.aggLATE$HAUL_MONTH < 10, paste0(0,df.aggLATE$HAUL_MONTH), df.aggLATE$HAUL_MONTH)[i]
  HAULYR = df.aggLATE$HAUL_YEAR[i]
  HAULLAT = df.aggLATE$LATITUDE[i]
  HAULLON = df.aggLATE$LON2[i]
  ## ping server
  GET(paste0(base.url,
             HAULYR,'-',HAULMO,'-01T00:00:00Z):1:(',HAULYR,'-',HAULMO,'-01T00:00:00Z)%5D%5B(',
             HAULLAT-0.2,'):1:(',HAULLAT+0.2,')%5D%5B(',HAULLON-0.2,'):1:(',HAULLON+0.2,')%5D'),
    write_disk(paste0(dump.file,i,'-110112P-sst.csv'), overwrite = T))
}

```
2018-2020 (Sea Surface Temperature, NOAA geopolar blended - Monthly, 2002-Present (2017
Reanalysis))
```{r, eval = F}
p <- proc.time() ## set timer
## Download NOAA Geopolar until present; first of each month. The 04102021P indicates 01 APR 2018 to Present. Only call for year & month in question for that row
df.aggLATE = subset(df.agg, HAUL_YEAR %in% 2019:2020 |
                      HAUL_MONTH > 03 & HAUL_YEAR == 2018)

base.url = 'https://oceanwatch.pifsc.noaa.gov/erddap/griddap/goes-poes-monthly-ghrsst-RAN.csv?analysed_sst%5B('
for(i in 1:nrow(df.aggLATE)){
  HAULMO = ifelse(df.aggLATE$HAUL_MONTH < 10, paste0(0,df.aggLATE$HAUL_MONTH), df.aggLATE$HAUL_MONTH)[i]
  HAULYR = df.aggLATE$HAUL_YEAR[i]
  HAULLAT = df.aggLATE$LATITUDE[i]
  HAULLON = df.aggLATE$LON2[i]
  ## ping server
  GET(paste0(base.url,
             HAULYR,'-',HAULMO,'-01T00:00:00Z):1:(',HAULYR,'-',HAULMO,'-01T00:00:00Z)%5D%5B(',
             HAULLAT-0.2,'):1:(',HAULLAT+0.2,')%5D%5B(',HAULLON-0.2,'):1:(',HAULLON+0.2,')%5D'),
    write_disk(paste0(dump.file,i,'-04012018P-sst.csv'), overwrite = T))
}

```
Bind all rows
```{r, eval = F}
## reset timer
p <- proc.time() 
## go through sst files
list.files(dump.file, pattern = "*sst.csv", full.names = TRUE) %>%
  ## open each CSV, ignore header
  lapply(FUN = read.csv, skip=2, header = F, na.strings = 'NaN') %>%
  ## bind them together
  bind_rows %>%
  ## skip NAs
  na.omit %>%
  ## write the large file
  write.csv(paste0(dump.file,"sstBind.csv"), row.names = FALSE) 
proc.time() - p
```
## Load SST CSV, fix headers and dates and re-save
```{r}
sstraw = read.csv(paste0(dump.file,'sstBind.csv'))
 names(sstraw) = c('UTC','LAT','LON','SSTDEGC')
 sstraw$DATE = as.Date(sstraw$UTC)
 sstraw$YEAR = year(sstraw$DATE)
 sstraw$MONTH = month(sstraw$DATE)
 sstraw$SSTDEGC<-ifelse(sstraw$SSTDEGC>100,sstraw$SSTDEGC-273,sstraw$SSTDEGC)
tail(sstraw)
 write.csv(sstraw,paste0(dump.file,'sstBind.csv'), row.names = F)
```
## Bind to SST
http://stackoverflow.com/questions/20590119/fuzzy-matching-of-coordinates
First you form the distance matrix. Each row in this matrix gives the distances from one of the catch locales to each of the SST observations. We use which.min to pick out the smallest entry in each row and then use this to index into the SST data. There are vector allocation problems when running this with the whole dataset, so I do it in two parts here.
A function that does the following:
  1. subset identical year+month combos from both the catch & sst dataframes (given by a matrix 'utp' with unique values in catch dataframe)
  2. calculate distance matrix using geosphere::distm between each lat-long point
  3. match each catch record with a corresponding sst record based on the minimum distance between the points. the subsetting step above ensures that that neighboring point was measured during that same month+year.
  4. bind all of these calculations back together into a master dataframe
  5. output record of which dates & how many records had to be discarded because they did not appear in the SST data
```{r}
 data = df
 sst = sstraw
bind.sst = function(data, sst){
agglist = list()
utp = data %>% group_by(HAUL_YEAR,HAUL_MONTH) %>% dplyr::summarise() 
for(u in 1:nrow(utp)){
    subDATA = subset(data, HAUL_MONTH == utp$HAUL_MONTH[u] & HAUL_YEAR == utp$HAUL_YEAR[u])
    subSST = subset(sst, MONTH == utp$HAUL_MONTH[u] & YEAR == utp$HAUL_YEAR[u])
      ## error trap for unmatched subscripts - happens with cloud cover. will just bounce to next iteration
      if(nrow(subSST) == 0) {
      print(paste0(utp$HAUL_MONTH[u]," ",utp$HAUL_YEAR[u]," dropped; absence of SST coverage; ", nrow(subDATA),' record(s) in all')) 
      next 
        }
    D = distm(subDATA[,c('LON2','LATITUDE')], subSST[,c('LON','LAT')])
    subfun = unlist(apply(D,1,which.min))
    agg0 = cbind(subDATA, subSST[subfun,])
    agglist[[u]] = agg0
  }
bigdata = do.call(rbind, agglist)
drops = nrow(df) - nrow(bigdata)
print(paste0(drops,' total records dropped, ', round(drops/nrow(df)*100, digits = 3)," % of total" )) ## see how many were dropped
return(bigdata)
}
```
Run the function
```{r}
## ~20 Mins to run
p = proc.time()
df_sst = bind.sst(df,sstraw)
proc.time() - p

head(df_sst)
unique(df_sst$SPECIES_COMMON_NAME)
summary(df_sst)
nrow(df_sst)
sum(is.na(df_sst$EYE_FORK))/nrow(df_sst)*100

# write.csv(df_sst,"G:/_LLDP/data/ERRDAP/sst_withnull.csv", row.names = F)
```


```{r, fig.height = 4, fig.width = 8}
# subset(df_sst, SWO_CPUE = NA) %>%  group_by(SPECIES_COMMON_NAME) %>% summarise(n = n())  
# subset(df_sst, SPECIES_COMMON_NAME == 'Swordfish')
# # ggplot(subset(df_sst, EYE_FORK = NA) %>% group_by(SPECIES_COMMON_NAME) %>% summarise(n = n()), aes(x = SPECIES_COMMON_NAME, y = n))+
# #   theme_bw()+
# #   geom_bar(stat = 'identity')
# # 
# # 
# # ggplot(df_sst, aes(x = LON2*-1, y = LATITUDE, col = SSTDEGC)) +
# #   theme_bw() +
# #   facet_wrap(~MONTH) + 
# #   geom_point()
# 
# tail(df_sst)
```
# Additional Code to add in PDO, SOI and ONI data
First, load your dataframe made above if necessary
```{r}
# df_sst = read.csv('G:/_LLDP/data/ERRDAP/sst_withnull.csv')
```

## PDO INDEX - dwnld 03 28 17 from ftp://ftp.ncdc.noaa.gov/pub/data/paleo/treering/reconstructions/pdo-macdonald2005.txt
and saved as CSV
```{r}
pdoi = read.csv('C:/Users/michelle.sculley/Documents/2022 MLS ASSESS/HI Data/Catch/PDOindex_10-21.csv', header = T)

pdoi$YEAR = substr(pdoi$Date,1,4)
pdoi$MONTH = substr(pdoi$Date,5,6)
pdoi$MONTH<-as.integer(pdoi$MONTH)
pdoi$YEAR<-as.integer(pdoi$YEAR)
 names(pdoi)[2] = 'PDO_INDEX'
write.csv(pdoi,'C:/Users/michelle.sculley/Documents/2022 MLS ASSESS/HI Data/Catch/PDO Index.csv', row.names = F)
# sstbind3 = merge(sstbind2,pdoi,by = c('YEAR', 'MONTH'))
df_sst_pdoi = merge(df_sst,pdoi,by.x = c('HAUL_YEAR','HAUL_MONTH'), by.y = c('YEAR', 'MONTH'))
```


## SOI index as proxy for ENSO
https://www.ncdc.noaa.gov/teleconnections/enso/indicators/soi/
"The Southern Oscillation Index (SOI) is a standardized index based on the observed sea level pressure differences between Tahiti and Darwin, Australia. The SOI is one measure of the large-scale fluctuations in air pressure occurring between the western and eastern tropical Pacific (i.e., the state of the Southern Oscillation) during El Ni?o and La Ni?a episodes. In general, smoothed time series of the SOI correspond very well with changes in ocean temperatures across the eastern tropical Pacific. The negative phase of the SOI represents below-normal air pressure at Tahiti and above-normal air pressure at Darwin. Prolonged periods of negative (positive) SOI values coincide with abnormally warm (cold) ocean waters across the eastern tropical Pacific typical of El Ni?o (La Ni?a) episodes. The methodology used to calculate SOI is available below. More information can be found at the Climate Prediction Center SOI page."
```{r}
soi = read.table('C:/Users/michelle.sculley/Documents/2022 MLS ASSESS/HI Data/Catch/SOI_10-21.txt', header = T,sep=",")
 soi$YEAR = as.integer(substr(soi$Date,1,4))
 soi$MONTH = as.integer(substr(soi$Date,5,6))
df_sst_pdoi_soi = merge(df_sst_pdoi,soi,by.x = c('HAUL_YEAR','HAUL_MONTH'), by.y = c('YEAR', 'MONTH'))
```
## ONI index
Link at bottom here
http://www.cpc.ncep.noaa.gov/products/analysis_monitoring/ensostuff/ONI_change.shtml

```{r, fig.height = 5, fig.width = 7}
nino = read.csv('C:/Users/michelle.sculley/Documents/2022 MLS ASSESS/HI Data/Catch/nino34_10-21.csv')
df_sst_pdoi_soi_oni = merge(df_sst_pdoi_soi,nino, by.x = c('HAUL_YEAR','HAUL_MONTH'), by.y = c('YR', 'MON'))
head(df_sst_pdoi_soi_oni)
tail(df_sst_pdoi_soi_oni)
unique(df_sst_pdoi_soi_oni$TARGET_SPECIES_NAME)
unique(df_sst_pdoi_soi_oni$SPECIES_COMMON_NAME)

df_sst_pdoi_soi_oni[3333:3350,]
nrow(df_sst_pdoi_soi_oni)

## SEVERAL MINUTES TO SAVE

write.csv(df_sst_pdoi_soi_oni,file='C:/Users/michelle.sculley/Documents/2022 MLS ASSESS/HI Data/Catch/MLS_CPUEwEnv.csv',row.names  = F)
```

Add Lunar Illumination to data file. From US Naval Observatory
As of October 12, 2021 - not able to update these files, portal seems to be broken?
```{r, fig.height = 5, fig.width = 7}
#df_sst_pdoi_soi_oni<-read.csv(file='C:/Users/michelle.sculley/Documents/2022 MLS ASSESS/HI Data/Catch/MLS_CPUEwEnv.csv')
Lunar = read.csv('C:/Users/michelle.sculley/Documents/Swordfish/DataPrep/LunarIllumination1990-2016.csv')
Lunar<-melt(Lunar,c("Month","Day"))
Lunar$variable<-gsub("X", "", paste(Lunar$variable))
Lunar$variable<-as.integer(Lunar$variable)
names(Lunar)<-c("Month","Day","Year","Illum")
Lunar$Year<-as.integer(Lunar$Year)
Lunar$Month<-ifelse(Lunar$Month=="Jan",1,
    ifelse(Lunar$Month=="Feb",2,
       ifelse(Lunar$Month=="Mar",3,
          ifelse(Lunar$Month=="April",4,
             ifelse(Lunar$Month=="May",5,
                ifelse(Lunar$Month=="June",6,
                   ifelse(Lunar$Month=="July",7,
                      ifelse(Lunar$Month=="August",8,
                         ifelse(Lunar$Month=="September",9,
                            ifelse(Lunar$Month=="October",10,
                              ifelse(Lunar$Month=="November",11,12)))))))))))
df_sst_pdoi_soi_Lunar = merge(df_sst_pdoi_soi,Lunar, by.x = c('HAUL_YEAR','HAUL_MONTH','HAUL_DAY'), by.y = c('Year', 'Month','Day'), all.x=TRUE, all.y=FALSE)
head(df_sst_pdoi_soi_Lunar)



## SEVERAL MINUTES TO SAVE

write.csv(df_sst_pdoi_soi_Lunar,file='C:/Users/michelle.sculley/Documents/Swordfish/DataPrep/SWO_CPUEwEnv.csv',row.names  = F)
```

```{r, fig.height = 5, fig.width = 7}
MLD = read.csv('G:/Swordfish/MLDFiles/MLD_Pacific.csv')
df_sst_pdoi_soi_Lunar<-read.csv('C:/Users/michelle.sculley/Documents/Swordfish/DataPrep/SWO_CPUEwEnv.csv')
MLD<-MLD[,2:6]
names(MLD)<-c("LAT","LON","MONTH","YEAR","MLD")
MLD$LON<-MLD$LON*-1
MLD<-subset(MLD,LON<=180&LON>=115)

#MLDTest<-subset(MLD,LON>160&LON<161&LAT>10&LAT<11)
#df_sst_pdoi_soi_LunarTest<-subset(df_sst_pdoi_soi_Lunar,LATITUDE>10&LATITUDE<11&LON2>160&LON<161)
#df_sst_pdoi_soi_Lunar$LON2<-df_sst_pdoi_soi_Lunar$LON2*-1
p = proc.time()
df_sst_pdoi_soi_Lunar_MLD = bind.sst(df_sst_pdoi_soi_oni,MLD)
proc.time() - p  
head(df_sst_pdoi_soi_Lunar_MLD)


## SEVERAL MINUTES TO SAVE

write.csv(df_sst_pdoi_soi_Lunar_MLD,file='C:/Users/michelle.sculley/Documents/2022 MLS ASSESS/HI Data/Catch/SWO_CPUEwMLD.csv',row.names  = F)

Model<-gam(SWO_CPUE~as.factor(HAUL_YEAR)+as.factor(HAUL_MONTH)+s(LATITUDE,LON2)+s(SSTDEGC)+s(Illum)+s(PDO)+s(SOI)+s(MLD),data=subset(df_sst_pdoi_soi_Lunar_MLD,TYPE_SET="D"))
```

