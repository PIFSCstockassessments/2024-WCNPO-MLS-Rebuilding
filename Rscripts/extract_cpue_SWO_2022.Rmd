---
title: "Download, QC & Reformat LL Observer Data"
author: "M Kapur, adapted from B Walsh (2013)"
date: "17 Feb 2017 Ongoing"
output:
  html_notebook:
    fig_caption: yes
    number_sections: yes
    toc: yes
    depth: 3
    self_contained: no
  html_document:
    number_sections: yes
    toc: yes
    depth: 3
    self_contained: no
---
```{r "setup", include=FALSE}
require("knitr")
opts_knit$set(root.dir = "G:/_LLDP")
```
<B> Background </b><br>
The previous incarnation of this protocol stated: "This file presents the commands used to prepare a data frame for use in a swordfish CPUE standardization to be conducted under ISC auspices in January 2013." I modify it here such that the user can extract, download, quality-control and reformat any Longline Data data in the procedure identical to that of B Walsh. This was created by combining a scripts left on the NOFISH server with materials from <b>NOAA Technical Memorandum NMFS-PIFSC-57, Walsh & Brodziak (2016)</b>. Previously, Walsh referred to the original dataframe as "Swordfish Data", but really this was a an amalgamation of Longline data, thus including all species (including bycatch). The principal edits to this material are: <br>
  1. Preserving field headings to match data stored in ORADATA, thus reducing confusion between data in and outputs  <br>
  2. Reducing code length/processing time though the use of pipes<br>
  3. Integrating the Oracle download, QA/QC, and data generation steps to improve documentation and reproducability<br>
  4. Including notes and instructions for each code block<br><Br>
Direct edits and questions to maia.kapur@noaa.gov.  
  
# Acquire data from Oracle
## Initial connection to the ORACLE data
This can alternatively be done manually via the Data Trawler. You need to ensure you have the Oracle software installed and the path to the PIC DSN in your Windows settings; requires ITS. Other obvious pre-req is that you have non-disclosure aggreements and access to the fisheries data.
```{r, warning = F, message = F}
## load packages
library(RODBC)
library(reshape2)
library(plyr)
library(dplyr)
library(tidyverse)



## designate username and password
UID = 'msculley'
pwd = 'BR4038%jwt5A'
dsn = "PIC"
con  =  RODBC::odbcConnect(dsn=dsn, uid=UID, pwd = pwd, believeNRows=FALSE)
 RODBC::odbcGetInfo(con)
```
## Download dataframes
Adapted from Walsh & Brodziak (2016), <i>Example A 2. Summary of data frame preparation for the PIFSC longline logbook database.</i> For ease, use the ORADATA schema. This is identical to the materials stored relationally in OPTD, but in this case it is a flat file & already has fields such as SEA_SURFACE_TEMPERATURE. The con object made above establishes the connection to the ORACLE database; use an SQL statement to extract the data you want.<B> You can alternatively download and save CSVs from Data Trawler, but you will likely time-out for large queries.</b>
```{r message = F, tidy=TRUE, warning = FALSE, width=70}
sqlTables(con, schema="ORADATA")
```
Show the alphabetized column headers available in a given table <i>before</i> downloading. The 1:10 truncates the list to the first 10 alphabetized column names.
```{r message = F, tidy=TRUE, warning = FALSE, width=70}
sort(sqlColumns(con, schema="ORADATA",sqtable="FC048A19")[,'COLUMN_NAME'])[1:10]
```
Use an SQL statement to query and download relevant data. You can use WHERE statements to narrow things down. This takes about 5 mins to run.
```{r message = F, eval = F, tidy=TRUE, warning = FALSE, width=70, eval = FALSE}
## Longline Data
p = proc.time()
logbook = sqlQuery(con, "SELECT * from ORADATA.FC048A19") ## FC048A19 is the same way it will appear on Data Trawler
ttr = proc.time() - p ## can use this to find out how long it took to download
# class(logbook) ## should be dataframe
# ## close connection
close(con)
```
You can save CSVs and/or assign downloaded table(s) to R objects. The logbook object is the full, downloaded dataset.<b> The naming scheme below was implemented by M Kapur and is retained throughout this document. It departs from that established by B Walsh.</b>
```{r}
#save(logbook ,file = 'logbook.rdata')
load('logbook.rdata')
setwd('C:\\Users\\michelle.sculley\\Documents\\2023 SWO ASSESS')
logbook<- read.csv("FC048A19_1990-2022_partial.csv")

## This is a species key I entered based on Logbook Reference Codes. It's alternatively availabile in one of the three relational tables in OPDT.
spcodes = read.csv('G:/sppcodes.csv')
```
# Aggregate by Set
## Create Unique Set Identifier
For data 1998 and later, the field SERIALNUM2 is the unique identifier for each set. For years prior to 1998, one must <b>construct</b> a unique identifier for each trip-set. Bill used a concatenation of the fields for landing year, trip number, and serial number of logbook to produce a unique identifier for each trip-set. <br><br>
The steps to do this are:<br>
+ Convert HAUL_YEAR, TRIP_NUMBER, and SERIAL_NUMBER2 from numeric to character (string) variables using as.character()<br>
+ Concatenate the three strings into a single string variable using paste0()<br>
```{r message = F, tidy=TRUE, warning = FALSE, width=70, eval = FALSE}
 length(unique(logbook$SERIAL_NUMBER2))

## If the haul year was 1998 or later, the logbook page number (aka the logbook page serial number) is the identifier. Otherwise generate & add concactenated variable using paste0()

logbook$SERIAL_NUMBER2 = as.numeric(logbook$SERIAL_NUMBER2)
logbook$UNIQUE_SET_ID = ifelse(logbook[,'HAUL_YEAR'] > 1997, logbook$SERIAL_NUMBER2, paste0(as.character(as.numeric(logbook[,'HAUL_YEAR'])), as.character(as.numeric(logbook[,'TRIP_NUMBER'])),as.character(as.numeric(logbook[,'SERIAL_NUMBER2']))))

length(unique(logbook$UNIQUE_SET_ID)) ## should be substantial and greater than unique values of SERIALNUM2
```
## Summarize
This is an involved yet necessary reshaping step to get the data in formats useful for CPUE analysis.
```{r, warning = F, message = F , eval = FALSE}
p = proc.time()
## if you want to do a 'test run', I recommend sub-sampling the dataframe

#logbook0 = logbook[sample(nrow(logbook), 0.05*nrow(logbook)), ]
logbook0 = logbook
#logbook0<-read.csv("C:/Users/michelle.sculley/Documents/MLS Stock Ass/HILL Data/HILL_logbook_083018.csv",header=TRUE)

## take out operational-only columns (no catch info) and remove duplicates so you have a single row of set-level information for each UNIQUE_SET_ID
operational <- subset(logbook0, !duplicated(logbook0[,'UNIQUE_SET_ID'])) 
operational <- subset(logbook0[ -c(58:64) ], !duplicated(logbook0[ -c(58:64) ][,'UNIQUE_SET_ID']))
## add in ENGLISH_NAME from code table
logbook0.join = left_join(logbook0, spcodes, by = 'SPECIES_CODE')

## change NAs in catch-related columns to 0
logbook0.join[,58:65][is.na(logbook0.join[,58:65])] = 0
proc.time() - p
```
# Aberrant Values
I included this as a pre-aggregation step that you may choose to skip. This focuses mainly on outstanding catch values and is designed to avoid erroneous CPUE calculations. Unusual lat/long values can be inspected visually.
```{r}
with(logbook0.join, summary(NUMBER_OF_FISH_KEPT))
subset(logbook0.join, NUMBER_OF_FISH_KEPT > 400)[,c('SERIAL_NUMBER2','HAUL_YEAR',
                                                         'SPECIES_CODE',
                                                         'NUMBER_OF_FISH_KEPT')]
## if you choose to drop all of the above, use the following. You can change it to a different threshold
logbook0.join = subset(logbook0.join, NUMBER_OF_FISH_KEPT < 400)

with(logbook0.join, summary(NUMBER_OF_SHARKS_FINNED))
subset(logbook0.join, NUMBER_OF_SHARKS_FINNED > 300)[,c('SERIAL_NUMBER2','HAUL_YEAR',
                                                         'SPECIES_CODE',
                                                         'NUMBER_OF_SHARKS_FINNED')]
## if you choose to drop all of the above, use the following. You can change it to a different threshold
 logbook0.join = subset(logbook0.join, NUMBER_OF_SHARKS_FINNED < 300)

with(logbook0.join, summary(NUMBER_OF_FISH_RELEASED))
subset(logbook0.join, NUMBER_OF_FISH_RELEASED > 300)[,c('SERIAL_NUMBER2','HAUL_YEAR',
                                                         'SPECIES_CODE',
                                                         'NUMBER_OF_FISH_RELEASED')]
## if you choose to drop all of the above, use the following. You can change it to a different threshold
logbook0.join = subset(logbook0.join, NUMBER_OF_FISH_RELEASED < 300)

with(logbook0.join, summary(NUMBER_RELEASED_ALIVE_UNINJURE))
subset(logbook0.join, NUMBER_RELEASED_ALIVE_UNINJURE > 300)[,c('SERIAL_NUMBER2','HAUL_YEAR',
                                                         'SPECIES_CODE',
                                                         'NUMBER_RELEASED_ALIVE_UNINJURE')]
## if you choose to drop all of the above, use the following. You can change it to a different threshold
logbook0.join = subset(logbook0.join, NUMBER_RELEASED_ALIVE_UNINJURE < 300)

with(logbook0.join, summary(NUMBER_RELEASED_DEAD))
subset(logbook0.join, NUMBER_RELEASED_DEAD > 300)[,c('SERIAL_NUMBER2','HAUL_YEAR',
                                                         'SPECIES_CODE',
                                                         'NUMBER_RELEASED_DEAD')]
## if you choose to drop all of the above, use the following. You can change it to a different threshold
logbook0.join = subset(logbook0.join, NUMBER_RELEASED_DEAD < 300)

with(logbook0.join, summary(NUMBER_RELEASED_INJURED))
subset(logbook0.join, NUMBER_RELEASED_INJURED > 300)[,c('SERIAL_NUMBER2','HAUL_YEAR',
                                                         'SPECIES_CODE',
                                                         'NUMBER_RELEASED_INJURED')]
## if you choose to drop all of the above, use the following. You can change it to a different threshold
logbook0.join = subset(logbook0.join, NUMBER_RELEASED_INJURED < 300)

```
This chain reshapes that data frame to produce a 'combination column' for each species (given by ENGLISH_NAME) and the appropriate category. This new dataframe is saved to catch_data_0
```{r, warning = F, message = F, eval = FALSE}
  library(data.table)
 #logbook0.join<- melt(logbook0.join[,c(58:60,62:64,117:118)],id.vars = c('UNIQUE_SET_ID','ENGLISH_NAME')) %>%
#     dcast(ENGLISH_NAME + variable ~ UNIQUE_SET_ID, fun.aggregate = NULL, value.var='value')
logbook0.join$ENGLISH_NAME<-as.character(logbook0.join$ENGLISH_NAME)
catch_df_0 = logbook0.join[,c(58:60,62:64,117:118)] %>% 
  gather(var, val, -(7:8)) %>% 
  unite("ENGLISH_NAME", ENGLISH_NAME, var) %>% 
  reshape(idvar = 'UNIQUE_SET_ID', timevar =  'ENGLISH_NAME', direction = 'wide')

## drop 'val.' from the column names (that gets added automatically in the unite() function)
names(catch_df_0)[-1] = substring(names(catch_df_0)[-1],5)

## check that length of unique IDS conserved
length(unique(logbook0$UNIQUE_SET_ID)) == length(unique(catch_df_0$UNIQUE_SET_ID))

 # catch_df_0 = group_by(UNIQUE_SET_ID, ENGLISH_NAME) %>% 
 # data.table::dcast( UNIQUE_SET_ID ~ ENGLISH_NAME + variable, fun.aggregate = NULL, value.var = 'value')
# proc.time() - p
```

```{r, warning = F, message = F, eval = FALSE}
## this step deletes columns where the sum of non-NA numbers is 0, e.g. 'NUMBER OF SHARKS FINNED' for a fish catch).
 #sum(catch_df_0$OPAH_NUMBER_OF_SHARKS_FINNED, na.rm = T) != 0
catch_df = catch_df_0[colSums(catch_df_0[-1], na.rm = T) == 0]
catch_df = catch_df_0
## add in the UID again
catch_df$UNIQUE_SET_ID = catch_df_0$UNIQUE_SET_ID
```
Some sanity checks
```{r, warning = F, message = F, eval = FALSE}
## you can inspect how many columns were dropped
ncol(catch_df_0) - ncol(catch_df)
## check how many unique set IDs in each (this should be TRUE)
length(unique(operational$UNIQUE_SET_ID)) == length(unique(catch_df$UNIQUE_SET_ID)) 
## check that very USID in catch_df also appears in operational (should be TRUE)
length(unique(catch_df$UNIQUE_SET_ID) %in% unique(operational$UNIQUE_SET_ID)) == length(unique(catch_df$UNIQUE_SET_ID)) 
## check that formatting was conserved
# unique(catch_df$UNIQUE_SET_ID)[1:10]
```
## Re-match Catch Data & Operational Info
This creates the final dataframe (df) that you will use for data cleaning and analysis.
```{r, warning = F, message = F, eval = FALSE}
df =  merge(catch_df, operational, by = 'UNIQUE_SET_ID')
dim(df)[1] == length(unique(logbook0$UNIQUE_SET_ID)) ## the final DF should have the same # of rows as UNIQUE_SET_ID
```
# Data Cleaning & QA/QC
## Average Hooks per Float
This is particularly important because hooks per float became the basis for two-sector management after 2001. 
```{r}
## If both HOOKS PER FLOAT values are NA, this will return an NA, otherwise calculates the mean.
df$HOOKS_PER_FLOAT =  ifelse(is.na(df$MAXIMUM_HOOKS_PER_FLOAT) & is.na(df$MINIMUM_HOOKS_PER_FLOAT), NA,
                             df$MAXIMUM_HOOKS_PER_FLOAT)
summary(df$HOOKS_PER_FLOAT)                     
```
## Identify Longline Sets
If a set was neither research (R) or experimental (X), assign it LL status. Note that the TYPE_SET column pre-assigns shallow (S) and deep (D) sets based on a 15-hook cutoff.
```{r}
# unique((df$RSCH_EXPMTL_CODE))
df$RSCH_EXPMTL_CODE = ifelse(is.na(df$RSCH_EXPMTL_CODE),"LL",df$RSCH_EXPMTL_CODE) 
```

## Define Position & Region
### Fix Coordinates
I copied the protocol used by B Walsh, wherein the <B>Begin Set & End Haul</b> latitudes and longitudes were averaged to create a singular coordinate point for each set. NA values are replaced with Begin Haul coordinates.
```{r}
## calculate means
df$LATITUDE = (df$BEGIN_SET_LATITUDE_DEGREES + df$END_HAUL_LATITUDE_DEGREES)/2
df$LONGITUDE = (df$BEGIN_SET_LONGITUDE_DEGREES + df$END_HAUL_LONGITUDE_DEGREES)/2
## replace NAs with Begin Haul coordinates
df$LATITUDE = ifelse(is.na(df$LATITUDE),df$BEGIN_HAUL_LATITUDE_DEGREES, df$LATITUDE)
df$LONGITUDE = ifelse(is.na(df$LONGITUDE),df$BEGIN_HAUL_LONGITUDE_DEGREES, df$LONGITUDE)
## you can throw up a quick map too see if the coords make sense. Keep in mind that LONGITUDE is in Deg W so you must multiply by -1 to match a normal map.
# with(df, plot(LONGITUDE*-1,LATITUDE, pch = 20))
```

### Designate Fishing Regions
Discretized and made into a factor (Brodziak & Walsh 2013)
```{r, warning = F, message = F}
attach(df)
df$REGION = ifelse(LATITUDE < 10 & LONGITUDE < 160, 1, NA)
df$REGION = ifelse(LATITUDE < 10 & LONGITUDE >= 160, 2, df$REGION)
df$REGION = ifelse(LATITUDE %in% 10:20 & LONGITUDE < 160, 3, df$REGION)
df$REGION = ifelse(LATITUDE %in% 10:20 & LONGITUDE >= 160, 4, df$REGION)
df$REGION = ifelse(LATITUDE %in% 20:30 & LONGITUDE < 160, 5, df$REGION)
df$REGION = ifelse(LATITUDE %in% 20:30 & LONGITUDE >= 160, 6, df$REGION)
df$REGION = ifelse(LATITUDE > 30 & LONGITUDE < 160, 7, df$REGION)
df$REGION = ifelse(LATITUDE > 30 & LONGITUDE >160, 8, df$REGION)
## plot same as above colored by region
with(df, plot(LONGITUDE*-1,LATITUDE, pch = 20, col = factor(REGION)))
legend('bottomright', legend = levels(factor(df$REGION)), pch = 20, col = levels(factor(df$REGION)))
```


## Optional: Summary data for a given spp
```{r}
## Sum fish kept and/or finned and/or released by set. You can choose your species using the sp.name or hard-code yourself. Make sure sp.name matches the way it appears in the data frame.
sp.name = 'SWORDFISH'
#df[,paste0(sp.name,'_NUMBER_OF_SHARKS_FINNED')]

df$SWO_NIND = rowSums(cbind(
  df[,paste0(sp.name,'_NUMBER_OF_SHARKS_FINNED')],
  df[,paste0(sp.name,'_NUMBER_OF_FISH_KEPT')],
  df[,paste0(sp.name,'_NUMBER_RELEASED_ALIVE_UNINJURE')],
  df[,paste0(sp.name,'_NUMBER_RELEASED_INJURED')],
  df[,paste0(sp.name,'_NUMBER_RELEASED_DEAD')], na.rm = T))

#View( cbind(subset(logbook, SERIAL_NUMBER2 == 133617)[,c('SERIAL_NUMBER2','HAUL_YEAR',
#                                                         'SPECIES_CODE',
 #                                                        'NUMBER_OF_SHARKS_FINNED','NUMBER_OF_FISH_KEPT')]))
## Divide NUMIND by HOOKS_PER_FLOAT
df$SWO_CPUE = (df$SWO_NIND/df$NUMBER_OF_HOOKS_SET)*1000
head(subset(df, SWO_CPUE > 0))$SWO_CPUE
nrow(subset(df, SWO_CPUE != 0))
hist(df$SWO_CPUE, xlim = c(0,200))



df_final<-subset(df,!is.na(HOOKS_PER_FLOAT))
df_final<-subset(df_final,!is.na(LATITUDE)&!is.na(LONGITUDE)) 
df_final<-subset(df_final,LONGITUDE>100)
df_final<-df_final[,-c(2:241)]



df<-df_final

df$SWO_CPUE<-ifelse(is.na(df$SWO_CPUE),0,df$SWO_CPUE)
df<-subset(df,!is.na(SET_YEAR))
df<-subset(df,!is.na(LONGITUDE))

df<-subset(df,NUMBER_OF_HOOKS_SET>=200)

df$BEGIN_SET_TIME<-ifelse(df$BEGIN_SET_TIME<1200,paste(0,df$BEGIN_SET_TIME,sep=""),df$BEGIN_SET_TIME)
 df$SET_MONTH<-ifelse(df$SET_MONTH<10,paste(0,df$SET_MONTH,sep=""),df$SET_MONTH)
 df$SET_DAY<-ifelse(df$SET_DAY<10,paste(0,df$SET_DAY,sep=""),df$SET_DAY)
  df$BEGIN_HAUL_TIME<-ifelse(df$BEGIN_HAUL_TIME<1200,paste(0,df$BEGIN_HAUL_TIME,sep=""),df$BEGIN_HAUL_TIME)
 df$HAUL_MONTH<-ifelse(df$HAUL_MONTH<10,paste(0,df$HAUL_MONTH,sep=""),df$HAUL_MONTH)
 df$HAUL_DAY<-ifelse(df$HAUL_DAY<10,paste(0,df$HAUL_DAY,sep=""),df$HAUL_DAY)
 df$BEGIN_TIME<-paste(df$SET_MONTH,"/",df$SET_DAY,"/",df$SET_YEAR," ",df$BEGIN_SET_TIME,sep="")


df$END_TIME<-paste(df$HAUL_MONTH,"/",df$HAUL_DAY,"/",df$HAUL_YEAR," ",df$BEGIN_HAUL_TIME,sep="")
df<-subset(df,!is.na(BEGIN_TIME))
df<-subset(df,!is.na(END_TIME))
df$BEGIN_TIME<-strptime(df$BEGIN_TIME,"%m/%d/%Y %H%M")
df$END_TIME<-strptime(df$END_TIME,"%m/%d/%Y %H%M")
df$SOAK_TIME<-difftime(df$END_TIME,df$BEGIN_TIME)/3600 ##converts to hours
df<-subset(df,!is.na(SOAK_TIME))  
df<-subset(df,SOAK_TIME>0&SOAK_TIME<48)
 write.csv(df_final,'C:\\Users\\michelle.sculley\\Documents\\2023 SWO ASSESS\\SWO_logbk_95_22.csv')
 df<-read.csv('C:\\Users\\michelle.sculley\\Documents\\2023 SWO ASSESS\\SWO_logbk_95_22.csv')
 
 ###### Add Enviromental data
library(lubridate)
library(httr)
library(ggplot2)
library(geosphere)
library(knitr)
library(dplyr)
df$LON2 = ifelse(df$LONGITUDE < 0, df$LONGITUDE*-1, df$LONGITUDE )
df$LON2=-df$LON2+360
df.agg = aggregate(cbind(LATITUDE,LON2) ~ HAUL_MONTH + HAUL_YEAR, data = df, FUN = mean)
### This will provide SST through 2018
base.url = 'https://oceanwatch.pifsc.noaa.gov/erddap/griddap/CRW_sst_v3_1_monthly.csvp?sea_surface_temperature[('
for(i in 1:nrow(df.agg)){
    HAULMO = ifelse(df.agg$HAUL_MONTH < 10, paste0(0,df.agg$HAUL_MONTH), df.agg$HAUL_MONTH)[i] ## designate month to download
    HAULYR = df.agg$HAUL_YEAR[i]
    HAULLAT = df.agg$LATITUDE[i]
    HAULLON = df.agg$LON2[i]
    ## ping server
    GET(paste0(base.url,
               HAULYR,'-',HAULMO,'-01T12:00:00Z):1:(',HAULYR,'-',HAULMO,'-01T12:00:00Z)][(',HAULLAT+10.2,'):1:(',HAULLAT-10.2,')][(',HAULLON-10.2,'):1:(',HAULLON+10.2,')]'),
        write_disk(paste0(getwd(),'/ERRDAP/',i,'-9422-sst.csv'), overwrite = T))
}




p <- proc.time() 
## go through sst files
Files<-list.files('C:/Users/Michelle.Sculley/Documents/2023 SWO ASSESS/ERRDAP/', pattern = "*sst.csv", full.names = TRUE) #%>%
    ## open each CSV, ignore header
    FileList<-lapply(Files, FUN = read.csv, header = T, na.strings = 'NaN') #%>%
    ## bind them together
   SSTbind<-bind_rows(FileList) #%>%
    ## skip NAs
  #na.omit %>%
    ## write the large file
    write.csv(SSTbind[,c(1:4)],"C:/Users/Michelle.Sculley/Documents/2023 SWO ASSESS/sstBind.csv", row.names = FALSE) 
proc.time() - p
sstraw<-SSTbind
rm(c(SSTbind,FileList,Files))
sstraw = read.csv('C:/Users/michelle.sculley/Documents/2023 SWO ASSESS/sstBind.csv')
 names(sstraw) = c('UTC','LAT','LON','SSTDEGC')
 #length(unique(paste0(sstraw$UTC,sstraw$LAT,sstraw$LON)))
 sstraw$DATE = as.Date(sstraw$UTC)
 sstraw$YEAR = year(sstraw$DATE)
 sstraw$MONTH = month(sstraw$DATE)
 sstraw$LON = (sstraw$LON-360)
 tail(sstraw)
 write.csv(sstraw,'C:/Users/michelle.sculley/Documents/2023 SWO ASSESS/sstBindF.csv', row.names = F)

bind.sst = function(data, sst){
    agglist = list()
    utp = data %>% group_by(HAUL_YEAR,HAUL_MONTH) %>% dplyr::summarise()
    
    for(u in 1:nrow(utp)){
        subDATA = subset(data, HAUL_MONTH == utp$HAUL_MONTH[u] & HAUL_YEAR == utp$HAUL_YEAR[u])
        subSST = subset(sst, MONTH == utp$HAUL_MONTH[u] & YEAR == utp$HAUL_YEAR[u])
        ## error trap for unmatched subscripts - happens with cloud cover. Will just bounce to next iteration
        if(nrow(subSST) == 0) {
            print(paste0(utp$HAUL_MONTH[u]," ",utp$HAUL_YEAR[u]," dropped; absence of SST coverage; ", nrow(subDATA),' record(s) in all')) 
            next }
        D = distm(subDATA[,c('LON2','LATITUDE')], subSST[,c('LON','LAT')])
        subfun = unlist(apply(D,1,which.min))
        agg0 = cbind(subDATA, subSST[subfun,])
        agglist[[u]] = agg0
    }
    bigdata = do.call(rbind, agglist)
    bigdata
    drops = nrow(df) - nrow(bigdata)
    print(paste0(drops,' total records dropped, ', round(drops/nrow(df)*100, digits = 3)," % of total" )) ## see how many were dropped
    return(bigdata)
}
p = proc.time()

df$LON2=df$LON2-360
df_sst = bind.sst(df,sstraw)
proc.time() - p
head(df_sst)
ggplot(df_sst, aes(x = LON2*-1, y = LATITUDE, col = SSTDEGC)) +
    theme_bw() +
    facet_wrap(~HAUL_MONTH) + 
    geom_point()

### Add PDO, SOI, ONI

pdoi = read.csv('PDO Index.csv', header = T)
df_sst_pdoi = merge(df_sst,pdoi[,c(1:3)],by.x = c('HAUL_YEAR','HAUL_MONTH'), by.y = c('YEAR', 'MONTH'))

soi = read.csv('SOI.csv', header = T)
df_sst_pdoi_soi = merge(df_sst_pdoi,soi,by.x = c('HAUL_YEAR','HAUL_MONTH'), by.y = c('YEAR', 'MONTH'))


nino = read.table('ONI.txt',header=TRUE)
df_sst_pdoi_soi_oni = merge(df_sst_pdoi_soi,nino[,c("YR","MON","ANOM")],by.x = c('HAUL_YEAR','HAUL_MONTH'), by.y = c('YR', 'MON')) 

write.csv(df_sst_pdoi_soi_oni, "SWOCPUE_Env.csv")


Lunar = read.csv('LunarIllumination1990-2022.csv')
Lunar<-melt(Lunar,c("Month","Day"))
Lunar$variable<-gsub("X", "", paste(Lunar$variable))
Lunar$variable<-as.integer(Lunar$variable)
names(Lunar)<-c("Month","Day","Year","Illum")
Lunar$Year<-as.integer(Lunar$Year)
Lunar$Month<-ifelse(Lunar$Month=="Jan",1,
    ifelse(Lunar$Month=="Feb",2,
       ifelse(Lunar$Month=="Mar",3,
          ifelse(Lunar$Month=="April",4,
             ifelse(Lunar$Month=="May",5,
                ifelse(Lunar$Month=="June",6,
                   ifelse(Lunar$Month=="July",7,
                      ifelse(Lunar$Month=="August",8,
                         ifelse(Lunar$Month=="September",9,
                            ifelse(Lunar$Month=="October",10,
                              ifelse(Lunar$Month=="November",11,12)))))))))))
df_sst_pdoi_soi_Lunar = merge(df_sst_pdoi_soi,Lunar, by.x = c('HAUL_YEAR','HAUL_MONTH','HAUL_DAY'), by.y = c('Year', 'Month','Day'), all.x=TRUE, all.y=FALSE)
head(df_sst_pdoi_soi_Lunar)



## SEVERAL MINUTES TO SAVE

write.csv(df_sst_pdoi_soi_Lunar,file='C:/Users/michelle.sculley/Documents/Swordfish/DataPrep/SWO_CPUEwEnv.csv',row.names  = F)
```

```{r, fig.height = 5, fig.width = 7}
MLD = read.csv('C:/Users/michelle.sculley/Documents/Swordfish/MLDFiles/MLD_Pacific.csv')
df_sst_pdoi_soi_Lunar<-read.csv('C:/Users/michelle.sculley/Documents/Swordfish/DataPrep/SWO_CPUEwEnv.csv')
#MLD<-MLD[,3:7]
names(MLD)<-c("LAT","LON","MONTH","YEAR","MLD")
MLD$LON<-MLD$LON*-1
MLD<-subset(MLD,LON<=180&LON>=115)
df_sst_pdoi_soi_Lunar$LON2<-df_sst_pdoi_soi_Lunar$LON2*-1
#MLDTest<-subset(MLD,LON>160&LON<161&LAT>10&LAT<11)
#df_sst_pdoi_soi_LunarTest<-subset(df_sst_pdoi_soi_Lunar,LATITUDE>10&LATITUDE<11&LON2>160&LON<161)
#df_sst_pdoi_soi_Lunar$LON2<-df_sst_pdoi_soi_Lunar$LON2*-1
p = proc.time()
df_sst_pdoi_soi_Lunar_MLD = bind.sst(df_sst_pdoi_soi_Lunar,MLD)
proc.time() - p  
head(df_sst_pdoi_soi_Lunar_MLD)


## SEVERAL MINUTES TO SAVE

write.csv(df_sst_pdoi_soi_Lunar_MLD,file='SWO_CPUEwEnv.csv',row.names  = F)

nino = read.table('ONI.txt',header=TRUE)
df_sst_pdoi_soi_Lunar_MLD_oni = merge(df_sst_pdoi_soi_Lunar_MLD,nino[,c("YR","MON","ANOM")],by.x = c('HAUL_YEAR','HAUL_MONTH'), by.y = c('YR', 'MON')) 

Final_DF<-df_sst_pdoi_soi_Lunar_MLD_oni[,-c(4,6,7,9,10,20,22,23,25:30,32:37,39:44,46:51,53:58,60,62:75,78:91,93:99,101:107,109,110,113,114,117,128:131)]
Final_DF$SWO_NIND<-ifelse(is.na(Final_DF$SWO_NIND),0,Final_DF$SWO_NIND)
Final_DF$SWO_CPUE<-ifelse(is.na(Final_DF$SWO_CPUE),0,Final_DF$SWO_CPUE)

write.csv(Final_DF,file='C:\\Users\\michelle.sculley\\Documents\\2023 SWO ASSESS\\SWO_CPUE_94_21.csv',row.names  = F)
Final_DF<-read.csv("C:\\Users\\michelle.sculley\\Documents\\2023 SWO ASSESS\\SWO_CPUE_94_21.csv")

Final_DF<-merge(df[,c("UNIQUE_SET_ID","LATITUDE","LONGITUDE")],Final_DF, by=c('UNIQUE_SET_ID'))

Model<-gam(SWO_CPUE~as.factor(HAUL_YEAR)+as.factor(HAUL_MONTH)+s(LATITUDE,LON2)+s(SSTDEGC)+s(Illum)+s(PDO)+s(SOI)+s(MLD),data=subset(df_sst_pdoi_soi_Lunar_MLD,TYPE_SET="D"))


Differences<-merge(df,Final_DF[,c("UNIQUE_SET_ID","SWO_CPUE")],by=c("UNIQUE_SET_ID"),all = TRUE)
Differences<-Differences[which(is.na(Differences$SWO_CPUE.y)),]

```


